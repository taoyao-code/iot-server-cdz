# Week 2: æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯æ–¹æ¡ˆ

> **åˆ¶å®šæ—¥æœŸ**: 2025-10-05  
> **æ‰§è¡Œå‘¨æœŸ**: 2-3å‘¨  
> **ä¼˜å…ˆçº§**: ğŸŸ¡ P1  
> **å‰ç½®æ¡ä»¶**: âœ… P0é—®é¢˜å·²ä¿®å¤

---

## ğŸ“‹ ç›®æ ‡æ¦‚è¿°

### æ ¸å¿ƒç›®æ ‡

1. **æå‡ååé‡**: ä¸‹è¡Œé˜Ÿåˆ—ä»PostgreSQLè¿ç§»åˆ°Redisï¼Œååæå‡10å€
2. **å¢å¼ºç¨³å®šæ€§**: å®ç°é™æµå’Œç†”æ–­æœºåˆ¶ï¼Œé˜²æ­¢èµ„æºè€—å°½
3. **ä¼˜åŒ–å»¶è¿Ÿ**: æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–ï¼Œå“åº”æ—¶é—´å‡å°‘50%
4. **å¼ºåŒ–è¿ç»´**: æ·±åº¦å¥åº·æ£€æŸ¥ï¼Œå¿«é€Ÿå®šä½é—®é¢˜

### é¢„æœŸæ”¶ç›Š

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æå‡ |
|-----|------|------|------|
| ä¸‹è¡Œæ¶ˆæ¯TPS | 100/s | 1000/s | 10x |
| å¹³å‡å“åº”å»¶è¿Ÿ | 100ms | 50ms | 50% |
| å¹¶å‘è¿æ¥æ•° | 1000 | 10000+ | 10x |
| ç³»ç»Ÿå¯ç”¨æ€§ | 99% | 99.9% | +0.9% |

---

## ğŸ¯ ä»»åŠ¡æ¸…å•

| # | ä»»åŠ¡ | å·¥ä½œé‡ | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|---|-----|--------|--------|------|
| 1 | Outboundé˜Ÿåˆ—RedisåŒ– | 5-7å¤© | é«˜ | â³ å¾…å¼€å§‹ |
| 2 | è¿æ¥é™æµå’Œç†”æ–­å™¨ | 2-3å¤© | é«˜ | â³ å¾…å¼€å§‹ |
| 3 | æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ– | 1-2å¤© | ä¸­ | â³ å¾…å¼€å§‹ |
| 4 | å¥åº·æ£€æŸ¥æ·±åº¦å¢å¼º | 1-2å¤© | ä¸­ | â³ å¾…å¼€å§‹ |

**æ€»å·¥ä½œé‡**: 9-14å¤©ï¼ˆå¯éƒ¨åˆ†å¹¶è¡Œï¼‰

---

## 1ï¸âƒ£ ä»»åŠ¡1: Outboundé˜Ÿåˆ—RedisåŒ–

### 1.1 é—®é¢˜åˆ†æ

#### å½“å‰æ¶æ„ç“¶é¢ˆ

```go
// å½“å‰å®ç°: internal/outbound/worker.go
type Worker struct {
    DB *pgxpool.Pool  // âŒ PostgreSQLä½œä¸ºæ¶ˆæ¯é˜Ÿåˆ—
}

// æ€§èƒ½é—®é¢˜:
// 1. æ¯ç§’æ‰«æDB: SELECT * FROM outbound_queue WHERE status=0
// 2. æ¯æ¡æ¶ˆæ¯4æ¬¡DBæ“ä½œ: INSERT â†’ UPDATE(status=1) â†’ UPDATE(status=2) â†’ DELETE
// 3. é‡è¯•é€»è¾‘ä¾èµ–DB: UPDATE retry_count, not_before
// 4. æ­»ä¿¡æ¸…ç†: DELETE FROM outbound_queue WHERE status=3
```

**æ€§èƒ½æµ‹è¯•æ•°æ®**:

```
PostgreSQLæ–¹æ¡ˆ:
- TPS: ~100æ¡/ç§’
- å»¶è¿Ÿ: P50=50ms, P99=500ms
- DBè´Ÿè½½: 400 QPS
- é”äº‰ç”¨: é«˜ï¼ˆSELECT FOR UPDATEï¼‰
```

#### ç›®æ ‡æ¶æ„

```
Redisæ–¹æ¡ˆ:
- TPS: ~1000æ¡/ç§’ (+10x)
- å»¶è¿Ÿ: P50=5ms, P99=50ms
- Redisè´Ÿè½½: 1000 QPS
- æ— é”: åŸºäºLIST/ZSETåŸå­æ“ä½œ
```

---

### 1.2 æŠ€æœ¯é€‰å‹

#### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | æ¨è |
|------|------|------|------|
| **Redis List** | ç®€å•ã€å¿«é€Ÿ | æ— ä¼˜å…ˆçº§ã€æ— å»¶è¿Ÿ | âŒ |
| **Redis Sorted Set** | æ”¯æŒå»¶è¿Ÿã€ä¼˜å…ˆçº§ | æ— åŸå­pop | âœ… æ¨è |
| **Redis Stream** | æŒä¹…åŒ–ã€æ¶ˆè´¹è€…ç»„ | å¤æ‚åº¦é«˜ | âš ï¸ å¤‡é€‰ |
| **RabbitMQ** | åŠŸèƒ½å®Œæ•´ | å¼•å…¥æ–°ç»„ä»¶ | âŒ |

**æœ€ç»ˆé€‰å‹**: **Redis Sorted Set + Hash**

---

### 1.3 æ•°æ®ç»“æ„è®¾è®¡

#### Redis Keyè®¾è®¡

```
# ä¸»é˜Ÿåˆ—ï¼ˆSorted Setï¼‰
outbound:queue        â†’ ZSET {member: msg_id, score: priority_timestamp}

# æ¶ˆæ¯è¯¦æƒ…ï¼ˆHashï¼‰
outbound:msg:{id}     â†’ HASH {phy_id, cmd, payload, retry_count, ...}

# å‘é€ä¸­é˜Ÿåˆ—ï¼ˆSorted Setï¼‰
outbound:pending      â†’ ZSET {member: msg_id, score: timeout_timestamp}

# æ­»ä¿¡é˜Ÿåˆ—ï¼ˆSorted Setï¼‰
outbound:dead         â†’ ZSET {member: msg_id, score: failed_timestamp}

# ç»Ÿè®¡è®¡æ•°å™¨
outbound:stats        â†’ HASH {sent_total, failed_total, pending_count}
```

#### æ¶ˆæ¯çŠ¶æ€æµè½¬

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¾…å‘é€    â”‚  ZADD outbound:queue
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚ Worker.Pop()
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‘é€ä¸­    â”‚  ZADD outbound:pending
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚ 
      â”œâ”€ æˆåŠŸ â”€â”€â†’ DEL outbound:msg:{id}
      â”‚
      â”œâ”€ å¤±è´¥ â”€â”€â†’ ZADD outbound:queue (é‡è¯•)
      â”‚
      â””â”€ è¶…æ—¶ â”€â”€â†’ ZADD outbound:dead (æ­»ä¿¡)
```

---

### 1.4 æ ¸å¿ƒå®ç°

#### 1.4.1 Redis Outbound Worker

```go
// internal/outbound/redis_worker.go
package outbound

import (
    "context"
    "encoding/json"
    "fmt"
    "time"

    "github.com/redis/go-redis/v9"
)

// RedisWorker Redisé˜Ÿåˆ—Worker
type RedisWorker struct {
    redis      *redis.Client
    interval   time.Duration
    maxRetries int
    throttle   time.Duration
    getConn    func(phyID string) (interface{}, bool)
}

// Message ä¸‹è¡Œæ¶ˆæ¯ç»“æ„
type Message struct {
    ID         string    `json:"id"`
    PhyID      string    `json:"phy_id"`
    Cmd        int       `json:"cmd"`
    Payload    []byte    `json:"payload"`
    Priority   int       `json:"priority"`     // 0-9ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
    RetryCount int       `json:"retry_count"`
    TimeoutSec int       `json:"timeout_sec"`
    CreatedAt  time.Time `json:"created_at"`
    LastError  string    `json:"last_error"`
}

// NewRedisWorker åˆ›å»ºRedis Worker
func NewRedisWorker(redis *redis.Client) *RedisWorker {
    return &RedisWorker{
        redis:      redis,
        interval:   time.Second,
        maxRetries: 3,
        throttle:   10 * time.Millisecond,  // æ¯æ¡æ¶ˆæ¯é—´éš”10ms
    }
}

// Run è¿è¡ŒWorker
func (w *RedisWorker) Run(ctx context.Context) error {
    ticker := time.NewTicker(w.interval)
    defer ticker.Stop()

    // ç«‹å³æ‰§è¡Œä¸€æ¬¡
    w.tick(ctx)

    for {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case <-ticker.C:
            w.tick(ctx)
        }
    }
}

// tick å•æ¬¡å¤„ç†å‘¨æœŸ
func (w *RedisWorker) tick(ctx context.Context) {
    // 1. æ‰«æè¶…æ—¶æ¶ˆæ¯
    w.sweepTimeouts(ctx)

    // 2. å¤„ç†å¾…å‘é€æ¶ˆæ¯
    w.processPending(ctx)

    // 3. æ¸…ç†æ­»ä¿¡
    w.cleanDead(ctx)
}

// processPending å¤„ç†å¾…å‘é€æ¶ˆæ¯
func (w *RedisWorker) processPending(ctx context.Context) {
    now := time.Now()
    maxScore := fmt.Sprintf("%d", now.Unix())

    // æ‰¹é‡è·å–å¾…å‘é€æ¶ˆæ¯ï¼ˆæŒ‰ä¼˜å…ˆçº§+æ—¶é—´æˆ³æ’åºï¼‰
    // ZRANGEBYSCORE outbound:queue -inf {now} LIMIT 0 100
    results, err := w.redis.ZRangeByScoreWithScores(ctx, "outbound:queue", &redis.ZRangeBy{
        Min:    "-inf",
        Max:    maxScore,
        Offset: 0,
        Count:  100,  // æ‰¹å¤„ç†å¤§å°
    }).Result()
    
    if err != nil || len(results) == 0 {
        return
    }

    for _, z := range results {
        msgID := z.Member.(string)
        
        // è·å–æ¶ˆæ¯è¯¦æƒ…
        msg, err := w.getMessage(ctx, msgID)
        if err != nil {
            continue
        }

        // å‘é€æ¶ˆæ¯
        if err := w.sendMessage(ctx, msg); err != nil {
            // å‘é€å¤±è´¥ï¼Œé‡è¯•
            w.retryMessage(ctx, msg, err.Error())
            continue
        }

        // å‘é€æˆåŠŸï¼Œç§»åˆ°pendingé˜Ÿåˆ—ç­‰å¾…ACK
        w.moveToPending(ctx, msg)
        
        // èŠ‚æµ
        time.Sleep(w.throttle)
    }
}

// sendMessage å‘é€æ¶ˆæ¯åˆ°è®¾å¤‡
func (w *RedisWorker) sendMessage(ctx context.Context, msg *Message) error {
    if w.getConn == nil {
        return fmt.Errorf("getConn not set")
    }

    conn, ok := w.getConn(msg.PhyID)
    if !ok {
        return fmt.Errorf("device %s offline", msg.PhyID)
    }

    writer, ok := conn.(interface {
        Write([]byte) error
        Protocol() string
    })
    if !ok {
        return fmt.Errorf("invalid conn type")
    }

    // æ ¹æ®åè®®æ„å»ºå¸§
    var frame []byte
    switch writer.Protocol() {
    case "bkv":
        frame = bkv.Build(uint16(msg.Cmd), 0, msg.PhyID, msg.Payload)
    default:
        frame = ap3000.Build(msg.PhyID, 0, byte(msg.Cmd), msg.Payload)
    }

    return writer.Write(frame)
}

// getMessage è·å–æ¶ˆæ¯è¯¦æƒ…
func (w *RedisWorker) getMessage(ctx context.Context, msgID string) (*Message, error) {
    key := fmt.Sprintf("outbound:msg:%s", msgID)
    data, err := w.redis.HGetAll(ctx, key).Result()
    if err != nil {
        return nil, err
    }

    msg := &Message{ID: msgID}
    // ååºåˆ—åŒ–...ï¼ˆçœç•¥ï¼‰
    return msg, nil
}

// retryMessage é‡è¯•æ¶ˆæ¯
func (w *RedisWorker) retryMessage(ctx context.Context, msg *Message, errMsg string) {
    msg.RetryCount++
    msg.LastError = errMsg

    if msg.RetryCount >= w.maxRetries {
        // ç§»åˆ°æ­»ä¿¡é˜Ÿåˆ—
        w.moveToDead(ctx, msg)
        return
    }

    // æŒ‡æ•°é€€é¿: 3^retryç§’åé‡è¯•
    delay := time.Duration(1<<uint(msg.RetryCount)) * 3 * time.Second
    nextTime := time.Now().Add(delay)

    // é‡æ–°åŠ å…¥é˜Ÿåˆ—
    score := w.calculateScore(msg.Priority, nextTime)
    w.redis.ZAdd(ctx, "outbound:queue", redis.Z{
        Score:  score,
        Member: msg.ID,
    })

    // æ›´æ–°æ¶ˆæ¯è¯¦æƒ…
    w.updateMessage(ctx, msg)
}

// calculateScore è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
// æ ¼å¼: {priority}{timestamp_seconds}
// ä¾‹å¦‚: ä¼˜å…ˆçº§5 + æ—¶é—´æˆ³1696500000 = 51696500000
func (w *RedisWorker) calculateScore(priority int, t time.Time) float64 {
    // ä¼˜å…ˆçº§èŒƒå›´0-9ï¼Œå æ®æœ€é«˜ä½
    return float64(priority)*1e12 + float64(t.Unix())
}

// moveToPending ç§»åˆ°pendingé˜Ÿåˆ—
func (w *RedisWorker) moveToPending(ctx context.Context, msg *Message) {
    // 1. ä»queueä¸­åˆ é™¤
    w.redis.ZRem(ctx, "outbound:queue", msg.ID)

    // 2. åŠ å…¥pendingé˜Ÿåˆ—ï¼ˆè¶…æ—¶æ—¶é—´æˆ³ï¼‰
    timeoutAt := time.Now().Add(time.Duration(msg.TimeoutSec) * time.Second)
    w.redis.ZAdd(ctx, "outbound:pending", redis.Z{
        Score:  float64(timeoutAt.Unix()),
        Member: msg.ID,
    })
}

// sweepTimeouts æ‰«æè¶…æ—¶æ¶ˆæ¯
func (w *RedisWorker) sweepTimeouts(ctx context.Context) {
    now := time.Now().Unix()
    maxScore := fmt.Sprintf("%d", now)

    // è·å–æ‰€æœ‰è¶…æ—¶æ¶ˆæ¯
    results, err := w.redis.ZRangeByScore(ctx, "outbound:pending", &redis.ZRangeBy{
        Min: "-inf",
        Max: maxScore,
    }).Result()
    
    if err != nil || len(results) == 0 {
        return
    }

    for _, msgID := range results {
        msg, err := w.getMessage(ctx, msgID)
        if err != nil {
            continue
        }

        // ä»pendingåˆ é™¤
        w.redis.ZRem(ctx, "outbound:pending", msgID)

        // é‡è¯•æˆ–æ­»ä¿¡
        w.retryMessage(ctx, msg, "ack timeout")
    }
}

// moveToDead ç§»åˆ°æ­»ä¿¡é˜Ÿåˆ—
func (w *RedisWorker) moveToDead(ctx context.Context, msg *Message) {
    // 1. ä»queue/pendingåˆ é™¤
    w.redis.ZRem(ctx, "outbound:queue", msg.ID)
    w.redis.ZRem(ctx, "outbound:pending", msg.ID)

    // 2. åŠ å…¥æ­»ä¿¡é˜Ÿåˆ—
    w.redis.ZAdd(ctx, "outbound:dead", redis.Z{
        Score:  float64(time.Now().Unix()),
        Member: msg.ID,
    })

    // 3. æ›´æ–°ç»Ÿè®¡
    w.redis.HIncrBy(ctx, "outbound:stats", "failed_total", 1)
}

// cleanDead æ¸…ç†è¿‡æœŸæ­»ä¿¡ï¼ˆä¿ç•™7å¤©ï¼‰
func (w *RedisWorker) cleanDead(ctx context.Context) {
    sevenDaysAgo := time.Now().AddDate(0, 0, -7).Unix()
    maxScore := fmt.Sprintf("%d", sevenDaysAgo)

    // åˆ é™¤7å¤©å‰çš„æ­»ä¿¡æ¶ˆæ¯
    w.redis.ZRemRangeByScore(ctx, "outbound:dead", "-inf", maxScore)
}

// updateMessage æ›´æ–°æ¶ˆæ¯è¯¦æƒ…
func (w *RedisWorker) updateMessage(ctx context.Context, msg *Message) {
    key := fmt.Sprintf("outbound:msg:%s", msg.ID)
    // åºåˆ—åŒ–å¹¶æ›´æ–°...ï¼ˆçœç•¥ï¼‰
}

// SetGetConn è®¾ç½®è¿æ¥è·å–å‡½æ•°
func (w *RedisWorker) SetGetConn(fn func(string) (interface{}, bool)) {
    w.getConn = fn
}
```

#### 1.4.2 æ¶ˆæ¯æ¨é€API

```go
// internal/outbound/redis_queue.go
package outbound

import (
    "context"
    "encoding/json"
    "fmt"
    "time"

    "github.com/google/uuid"
    "github.com/redis/go-redis/v9"
)

// RedisQueue Redisæ¶ˆæ¯é˜Ÿåˆ—
type RedisQueue struct {
    redis *redis.Client
}

// NewRedisQueue åˆ›å»ºé˜Ÿåˆ—
func NewRedisQueue(redis *redis.Client) *RedisQueue {
    return &RedisQueue{redis: redis}
}

// Push æ¨é€æ¶ˆæ¯åˆ°é˜Ÿåˆ—
func (q *RedisQueue) Push(ctx context.Context, msg *Message) (string, error) {
    // 1. ç”Ÿæˆæ¶ˆæ¯ID
    if msg.ID == "" {
        msg.ID = uuid.New().String()
    }
    msg.CreatedAt = time.Now()

    // 2. å­˜å‚¨æ¶ˆæ¯è¯¦æƒ…
    msgKey := fmt.Sprintf("outbound:msg:%s", msg.ID)
    msgData, err := json.Marshal(msg)
    if err != nil {
        return "", err
    }
    
    if err := q.redis.HSet(ctx, msgKey, "data", msgData).Err(); err != nil {
        return "", err
    }

    // 3. åŠ å…¥é˜Ÿåˆ—ï¼ˆè®¡ç®—åˆ†æ•°ï¼šä¼˜å…ˆçº§+æ—¶é—´æˆ³ï¼‰
    score := q.calculateScore(msg.Priority, msg.CreatedAt)
    if err := q.redis.ZAdd(ctx, "outbound:queue", redis.Z{
        Score:  score,
        Member: msg.ID,
    }).Err(); err != nil {
        return "", err
    }

    // 4. æ›´æ–°ç»Ÿè®¡
    q.redis.HIncrBy(ctx, "outbound:stats", "queued_total", 1)

    return msg.ID, nil
}

// Ack ç¡®è®¤æ¶ˆæ¯å·²å¤„ç†
func (q *RedisQueue) Ack(ctx context.Context, msgID string) error {
    // 1. ä»pendingåˆ é™¤
    if err := q.redis.ZRem(ctx, "outbound:pending", msgID).Err(); err != nil {
        return err
    }

    // 2. åˆ é™¤æ¶ˆæ¯è¯¦æƒ…
    msgKey := fmt.Sprintf("outbound:msg:%s", msgID)
    if err := q.redis.Del(ctx, msgKey).Err(); err != nil {
        return err
    }

    // 3. æ›´æ–°ç»Ÿè®¡
    q.redis.HIncrBy(ctx, "outbound:stats", "completed_total", 1)

    return nil
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (q *RedisQueue) GetStats(ctx context.Context) (map[string]string, error) {
    return q.redis.HGetAll(ctx, "outbound:stats").Result()
}

// GetQueueSize è·å–é˜Ÿåˆ—é•¿åº¦
func (q *RedisQueue) GetQueueSize(ctx context.Context) (int64, error) {
    return q.redis.ZCard(ctx, "outbound:queue").Result()
}

// GetPendingSize è·å–pendingé•¿åº¦
func (q *RedisQueue) GetPendingSize(ctx context.Context) (int64, error) {
    return q.redis.ZCard(ctx, "outbound:pending").Result()
}

// GetDeadSize è·å–æ­»ä¿¡é•¿åº¦
func (q *RedisQueue) GetDeadSize(ctx context.Context) (int64, error) {
    return q.redis.ZCard(ctx, "outbound:dead").Result()
}

// calculateScore åŒRedisWorker
func (q *RedisQueue) calculateScore(priority int, t time.Time) float64 {
    return float64(priority)*1e12 + float64(t.Unix())
}
```

---

### 1.5 æ•°æ®è¿ç§»æ–¹æ¡ˆ

#### è¿ç§»ç­–ç•¥ï¼šåŒå†™ + ç°åº¦åˆ‡æ¢

```
é˜¶æ®µ1: å‡†å¤‡é˜¶æ®µï¼ˆ0.5å¤©ï¼‰
â”œâ”€ Rediséƒ¨ç½²å’Œæµ‹è¯•
â”œâ”€ ä»£ç å‡†å¤‡ï¼ˆRedisWorker + RedisQueueï¼‰
â””â”€ é…ç½®å¼€å…³ï¼ˆoutbound.backend: pg / redis / dualï¼‰

é˜¶æ®µ2: åŒå†™é˜¶æ®µï¼ˆ1å¤©ï¼‰
â”œâ”€ é…ç½®: backend = "dual"
â”œâ”€ åŒæ—¶å†™å…¥PGå’ŒRedis
â”œâ”€ ä»…ä»PGæ¶ˆè´¹ï¼ˆä¿æŒç°çŠ¶ï¼‰
â””â”€ ç›‘æ§Redisæ•°æ®ä¸€è‡´æ€§

é˜¶æ®µ3: ç°åº¦åˆ‡æ¢ï¼ˆ1å¤©ï¼‰
â”œâ”€ é…ç½®: backend = "redis"
â”œâ”€ ä»Redisæ¶ˆè´¹
â”œâ”€ ä¿ç•™PGé˜Ÿåˆ—ä½œä¸ºå¤‡ä»½
â””â”€ ç›‘æ§æ€§èƒ½å’Œé”™è¯¯ç‡

é˜¶æ®µ4: æ¸…ç†é˜¶æ®µï¼ˆ0.5å¤©ï¼‰
â”œâ”€ ç¡®è®¤Redisç¨³å®šè¿è¡Œ3å¤©
â”œâ”€ åœæ­¢PGå†™å…¥
â”œâ”€ æ¸…ç†PG outbound_queueè¡¨æ•°æ®
â””â”€ åˆ é™¤æ—§Workerä»£ç 
```

#### åŒå†™å®ç°

```go
// internal/outbound/dual_queue.go
package outbound

import (
    "context"
)

// DualQueue åŒå†™é˜Ÿåˆ—ï¼ˆPG + Redisï¼‰
type DualQueue struct {
    pg    *PostgresQueue
    redis *RedisQueue
}

func NewDualQueue(pg *PostgresQueue, redis *RedisQueue) *DualQueue {
    return &DualQueue{pg: pg, redis: redis}
}

// Push åŒæ—¶å†™å…¥PGå’ŒRedis
func (q *DualQueue) Push(ctx context.Context, msg *Message) (string, error) {
    // 1. å†™å…¥PGï¼ˆä¿åº•ï¼‰
    pgID, err := q.pg.Push(ctx, msg)
    if err != nil {
        return "", err
    }

    // 2. å†™å…¥Redisï¼ˆå¼‚æ­¥ï¼Œå¤±è´¥ä¸å½±å“ï¼‰
    go func() {
        redisID, err := q.redis.Push(context.Background(), msg)
        if err != nil {
            // è®°å½•å‘Šè­¦ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹
            log.Warn("redis push failed", zap.Error(err))
        } else {
            // éªŒè¯ä¸€è‡´æ€§
            if pgID != redisID {
                log.Error("id mismatch", zap.String("pg", pgID), zap.String("redis", redisID))
            }
        }
    }()

    return pgID, nil
}
```

---

### 1.6 æ€§èƒ½æµ‹è¯•

#### æµ‹è¯•åœºæ™¯

```bash
# åœºæ™¯1: ååé‡æµ‹è¯•ï¼ˆå‘é€1ä¸‡æ¡æ¶ˆæ¯ï¼‰
go run test/benchmark/outbound_throughput.go \
  --backend=redis \
  --count=10000 \
  --concurrency=10

# æœŸæœ›ï¼š
# - PG: ~100æ¡/ç§’ï¼Œè€—æ—¶100ç§’
# - Redis: ~1000æ¡/ç§’ï¼Œè€—æ—¶10ç§’

# åœºæ™¯2: å»¶è¿Ÿæµ‹è¯•ï¼ˆP50/P95/P99ï¼‰
go run test/benchmark/outbound_latency.go \
  --backend=redis \
  --duration=60s

# æœŸæœ›ï¼š
# - PG: P50=50ms, P99=500ms
# - Redis: P50=5ms, P99=50ms

# åœºæ™¯3: ç¨³å®šæ€§æµ‹è¯•ï¼ˆæŒç»­24å°æ—¶ï¼‰
go run test/stability/outbound_stress.go \
  --backend=redis \
  --tps=500 \
  --duration=24h

# æœŸæœ›ï¼š
# - é”™è¯¯ç‡ < 0.1%
# - å†…å­˜ç¨³å®šï¼ˆæ— æ³„éœ²ï¼‰
# - Redisè¿æ¥æ± æ­£å¸¸
```

---

### 1.7 å›æ»šæ–¹æ¡ˆ

```go
// é…ç½®å›æ»š
# configs/production.yaml
outbound:
  backend: "pg"  # ç«‹å³åˆ‡å›PG

// æ•°æ®å›æ»š
// Redisä¸­æœªå®Œæˆçš„æ¶ˆæ¯è¿ç§»å›PG
redis-cli --scan --pattern "outbound:msg:*" | while read key; do
  // è¯»å–æ¶ˆæ¯å¹¶å†™å…¥PG
done
```

---

### 1.8 ç›‘æ§æŒ‡æ ‡

```go
// internal/metrics/outbound.go

type OutboundMetrics struct {
    // é˜Ÿåˆ—é•¿åº¦
    QueueSize   prometheus.Gauge  // outbound_queue_size
    PendingSize prometheus.Gauge  // outbound_pending_size
    DeadSize    prometheus.Gauge  // outbound_dead_size
    
    // ååé‡
    SentTotal      prometheus.Counter  // outbound_sent_total
    CompletedTotal prometheus.Counter  // outbound_completed_total
    FailedTotal    prometheus.Counter  // outbound_failed_total
    
    // å»¶è¿Ÿ
    SendLatency prometheus.Histogram  // outbound_send_latency_seconds
    AckLatency  prometheus.Histogram  // outbound_ack_latency_seconds
    
    // é”™è¯¯
    TimeoutTotal  prometheus.Counter  // outbound_timeout_total
    ErrorsTotal   prometheus.CounterVec  // outbound_errors_total{type}
}
```

---

## 2ï¸âƒ£ ä»»åŠ¡2: è¿æ¥é™æµå’Œç†”æ–­å™¨

### 2.1 é—®é¢˜åˆ†æ

#### å½“å‰é£é™©

```go
// internal/tcpserver/server.go
func (s *Server) Start() error {
    for {
        conn, _ := s.ln.Accept()  // âŒ æ— é™æ¥å—è¿æ¥
        go s.handleConn(conn)      // âŒ goroutineæ— é™å¢é•¿
    }
}

// é£é™©ï¼š
// 1. DDoSæ”»å‡»ï¼šç¬é—´10ä¸‡è¿æ¥ï¼ŒOOMå´©æºƒ
// 2. æ…¢å®¢æˆ·ç«¯ï¼šå¤§é‡goroutineé˜»å¡ï¼Œè°ƒåº¦å™¨é¥±å’Œ
// 3. èµ„æºè€—å°½ï¼šæ–‡ä»¶æè¿°ç¬¦ã€å†…å­˜ã€CPUå…¨éƒ¨è€—å°½
// 4. é›ªå´©æ•ˆåº”ï¼šä¸€å°æœºå™¨å´©æºƒï¼Œæµé‡è½¬ç§»å¯¼è‡´å…¶ä»–æœºå™¨å´©æºƒ
```

---

### 2.2 é™æµå™¨è®¾è®¡

#### 2.2.1 è¿æ¥æ•°é™æµï¼ˆSemaphoreï¼‰

```go
// internal/tcpserver/limiter.go
package tcpserver

import (
    "context"
    "fmt"
    "time"
)

// ConnectionLimiter è¿æ¥æ•°é™æµå™¨
type ConnectionLimiter struct {
    sem      chan struct{}
    timeout  time.Duration
    metrics  *LimiterMetrics
}

func NewConnectionLimiter(maxConn int, timeout time.Duration) *ConnectionLimiter {
    return &ConnectionLimiter{
        sem:     make(chan struct{}, maxConn),
        timeout: timeout,
    }
}

// Acquire è·å–è¿æ¥è®¸å¯
func (l *ConnectionLimiter) Acquire(ctx context.Context) error {
    ctx, cancel := context.WithTimeout(ctx, l.timeout)
    defer cancel()

    select {
    case l.sem <- struct{}{}:
        l.metrics.ActiveConnections.Inc()
        return nil
    case <-ctx.Done():
        l.metrics.RejectedConnections.Inc()
        return fmt.Errorf("connection limit exceeded")
    }
}

// Release é‡Šæ”¾è¿æ¥è®¸å¯
func (l *ConnectionLimiter) Release() {
    <-l.sem
    l.metrics.ActiveConnections.Dec()
}

// Current å½“å‰è¿æ¥æ•°
func (l *ConnectionLimiter) Current() int {
    return len(l.sem)
}

// Available å¯ç”¨è¿æ¥æ•°
func (l *ConnectionLimiter) Available() int {
    return cap(l.sem) - len(l.sem)
}
```

#### 2.2.2 é€Ÿç‡é™æµï¼ˆToken Bucketï¼‰

```go
// internal/tcpserver/rate_limiter.go
package tcpserver

import (
    "golang.org/x/time/rate"
)

// RateLimiter åŸºäºToken Bucketçš„é€Ÿç‡é™æµå™¨
type RateLimiter struct {
    limiter *rate.Limiter
    burst   int
}

// NewRateLimiter åˆ›å»ºé€Ÿç‡é™æµå™¨
// rate: æ¯ç§’å…è®¸çš„è¯·æ±‚æ•°
// burst: çªå‘å®¹é‡
func NewRateLimiter(ratePerSec int, burst int) *RateLimiter {
    return &RateLimiter{
        limiter: rate.NewLimiter(rate.Limit(ratePerSec), burst),
        burst:   burst,
    }
}

// Allow æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
func (l *RateLimiter) Allow() bool {
    return l.limiter.Allow()
}

// Wait ç­‰å¾…ç›´åˆ°å…è®¸è¯·æ±‚ï¼ˆé˜»å¡ï¼‰
func (l *RateLimiter) Wait(ctx context.Context) error {
    return l.limiter.Wait(ctx)
}
```

---

### 2.3 ç†”æ–­å™¨è®¾è®¡

#### 2.3.1 ç†”æ–­å™¨çŠ¶æ€æœº

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Closed   â”‚  æ­£å¸¸çŠ¶æ€ï¼Œå…è®¸è¯·æ±‚é€šè¿‡
â”‚ (æ­£å¸¸è¿è¡Œ)  â”‚  å¤±è´¥ç‡æ£€æµ‹
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚ å¤±è´¥ç‡ > é˜ˆå€¼
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Open    â”‚  ç†”æ–­çŠ¶æ€ï¼Œæ‹’ç»æ‰€æœ‰è¯·æ±‚
â”‚  (ç†”æ–­ä¸­)   â”‚  å®šæ—¶å™¨ï¼š30ç§’
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚ è¶…æ—¶
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Half-Open  â”‚  åŠå¼€çŠ¶æ€ï¼Œå…è®¸å°‘é‡è¯·æ±‚
â”‚  (è¯•æ¢ä¸­)   â”‚  æˆåŠŸåˆ™æ¢å¤ï¼Œå¤±è´¥åˆ™ç»§ç»­ç†”æ–­
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ æˆåŠŸ â†’ Closed
       â””â”€ å¤±è´¥ â†’ Open
```

#### 2.3.2 ç†”æ–­å™¨å®ç°

```go
// internal/tcpserver/circuit_breaker.go
package tcpserver

import (
    "errors"
    "sync"
    "time"
)

type State int

const (
    StateClosed State = iota
    StateOpen
    StateHalfOpen
)

// CircuitBreaker ç†”æ–­å™¨
type CircuitBreaker struct {
    mu            sync.RWMutex
    state         State
    failureCount  int
    successCount  int
    lastFailTime  time.Time
    
    // é…ç½®
    threshold     int           // å¤±è´¥æ¬¡æ•°é˜ˆå€¼
    timeout       time.Duration // ç†”æ–­è¶…æ—¶
    halfOpenMax   int           // åŠå¼€çŠ¶æ€æœ€å¤§è¯·æ±‚æ•°
    
    // å›è°ƒ
    onStateChange func(from, to State)
}

// NewCircuitBreaker åˆ›å»ºç†”æ–­å™¨
func NewCircuitBreaker(threshold int, timeout time.Duration) *CircuitBreaker {
    return &CircuitBreaker{
        state:       StateClosed,
        threshold:   threshold,
        timeout:     timeout,
        halfOpenMax: 5,
    }
}

var ErrCircuitOpen = errors.New("circuit breaker is open")

// Call æ‰§è¡Œå‡½æ•°ï¼Œå—ç†”æ–­å™¨ä¿æŠ¤
func (cb *CircuitBreaker) Call(fn func() error) error {
    if !cb.allow() {
        return ErrCircuitOpen
    }

    err := fn()
    cb.record(err)
    
    return err
}

// allow æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
func (cb *CircuitBreaker) allow() bool {
    cb.mu.RLock()
    defer cb.mu.RUnlock()

    switch cb.state {
    case StateClosed:
        return true
    
    case StateOpen:
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼Œå¯ä»¥è¿›å…¥åŠå¼€çŠ¶æ€
        if time.Since(cb.lastFailTime) > cb.timeout {
            cb.mu.RUnlock()
            cb.mu.Lock()
            cb.setState(StateHalfOpen)
            cb.failureCount = 0
            cb.successCount = 0
            cb.mu.Unlock()
            cb.mu.RLock()
            return true
        }
        return false
    
    case StateHalfOpen:
        // åŠå¼€çŠ¶æ€ï¼Œé™åˆ¶è¯·æ±‚æ•°
        return cb.successCount + cb.failureCount < cb.halfOpenMax
    
    default:
        return false
    }
}

// record è®°å½•è¯·æ±‚ç»“æœ
func (cb *CircuitBreaker) record(err error) {
    cb.mu.Lock()
    defer cb.mu.Unlock()

    if err != nil {
        // å¤±è´¥
        cb.failureCount++
        cb.lastFailTime = time.Now()

        switch cb.state {
        case StateClosed:
            if cb.failureCount >= cb.threshold {
                cb.setState(StateOpen)
            }
        
        case StateHalfOpen:
            // åŠå¼€çŠ¶æ€å¤±è´¥ï¼Œç«‹å³ç†”æ–­
            cb.setState(StateOpen)
        }
    } else {
        // æˆåŠŸ
        cb.successCount++

        switch cb.state {
        case StateHalfOpen:
            // åŠå¼€çŠ¶æ€æˆåŠŸï¼Œæ¢å¤æ­£å¸¸
            if cb.successCount >= cb.halfOpenMax/2 {
                cb.setState(StateClosed)
                cb.failureCount = 0
                cb.successCount = 0
            }
        
        case StateClosed:
            // æ­£å¸¸çŠ¶æ€æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
            if cb.successCount > 0 && cb.successCount%100 == 0 {
                cb.failureCount = 0
            }
        }
    }
}

// setState çŠ¶æ€è½¬æ¢
func (cb *CircuitBreaker) setState(newState State) {
    if cb.state == newState {
        return
    }

    oldState := cb.state
    cb.state = newState

    if cb.onStateChange != nil {
        cb.onStateChange(oldState, newState)
    }
}

// State è·å–å½“å‰çŠ¶æ€
func (cb *CircuitBreaker) State() State {
    cb.mu.RLock()
    defer cb.mu.RUnlock()
    return cb.state
}

// Metrics è·å–æŒ‡æ ‡
func (cb *CircuitBreaker) Metrics() (state State, failures int, successes int) {
    cb.mu.RLock()
    defer cb.mu.RUnlock()
    return cb.state, cb.failureCount, cb.successCount
}
```

---

### 2.4 é›†æˆåˆ°TCP Server

```go
// internal/tcpserver/server.go

type Server struct {
    // ... ç°æœ‰å­—æ®µ
    
    // é™æµå’Œç†”æ–­
    connLimiter  *ConnectionLimiter
    rateLimiter  *RateLimiter
    breaker      *CircuitBreaker
}

func NewServer(cfg *Config) *Server {
    s := &Server{
        // ...
        connLimiter: NewConnectionLimiter(cfg.MaxConnections, 5*time.Second),
        rateLimiter: NewRateLimiter(cfg.RateLimit, cfg.RateBurst),
        breaker:     NewCircuitBreaker(cfg.BreakerThreshold, cfg.BreakerTimeout),
    }
    
    // ç†”æ–­å™¨çŠ¶æ€å˜åŒ–å›è°ƒ
    s.breaker.onStateChange = func(from, to State) {
        log.Warn("circuit breaker state changed",
            zap.String("from", stateString(from)),
            zap.String("to", stateString(to)),
        )
        
        // å‘é€å‘Šè­¦
        if to == StateOpen {
            alert.Send("Circuit Breaker Opened", "TCP server is experiencing high failure rate")
        }
    }
    
    return s
}

func (s *Server) Start() error {
    for {
        // 1. é€Ÿç‡é™æµ
        if !s.rateLimiter.Allow() {
            time.Sleep(10 * time.Millisecond)
            continue
        }

        // 2. æ¥å—è¿æ¥
        conn, err := s.ln.Accept()
        if err != nil {
            if isTemporaryError(err) {
                continue
            }
            return err
        }

        // 3. è¿æ¥æ•°é™æµ
        if err := s.connLimiter.Acquire(context.Background()); err != nil {
            conn.Close()
            s.metrics.ConnectionsRejected.Inc()
            continue
        }

        // 4. ç†”æ–­å™¨æ£€æŸ¥
        err = s.breaker.Call(func() error {
            // å¤„ç†è¿æ¥
            go s.handleConnWithProtection(conn)
            return nil
        })

        if err == ErrCircuitOpen {
            conn.Close()
            s.connLimiter.Release()
            s.metrics.ConnectionsCircuitBroken.Inc()
            continue
        }
    }
}

// handleConnWithProtection å¸¦ä¿æŠ¤çš„è¿æ¥å¤„ç†
func (s *Server) handleConnWithProtection(conn net.Conn) {
    defer s.connLimiter.Release()
    defer conn.Close()
    defer func() {
        if r := recover(); r != nil {
            log.Error("panic in handleConn", zap.Any("panic", r))
            s.breaker.record(fmt.Errorf("panic: %v", r))
        }
    }()

    // è°ƒç”¨åŸæœ‰çš„handleConn
    err := s.handleConn(conn)
    if err != nil {
        s.breaker.record(err)
    } else {
        s.breaker.record(nil)
    }
}
```

---

### 2.5 é…ç½®ç®¡ç†

```yaml
# configs/example.yaml

tcp:
  addr: ":9999"
  
  # é™æµé…ç½®
  max_connections: 10000        # æœ€å¤§å¹¶å‘è¿æ¥æ•°
  rate_limit: 100               # æ¯ç§’æ¥å—è¿æ¥æ•°
  rate_burst: 200               # çªå‘å®¹é‡
  
  # ç†”æ–­å™¨é…ç½®
  breaker:
    threshold: 50               # å¤±è´¥æ¬¡æ•°é˜ˆå€¼
    timeout: 30s                # ç†”æ–­è¶…æ—¶
    half_open_max: 5            # åŠå¼€çŠ¶æ€æµ‹è¯•è¯·æ±‚æ•°
```

---

## 3ï¸âƒ£ ä»»åŠ¡3: æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–

### 3.1 é—®é¢˜åˆ†æ

#### æ…¢æŸ¥è¯¢è¯†åˆ«

```sql
-- æŸ¥çœ‹æ…¢æŸ¥è¯¢ï¼ˆ>100msï¼‰
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 100
ORDER BY total_exec_time DESC
LIMIT 20;

-- å¸¸è§æ…¢æŸ¥è¯¢ï¼š
-- 1. SELECT * FROM devices WHERE last_seen_at > NOW() - INTERVAL '5 minutes'
--    (æ— ç´¢å¼•ï¼Œå…¨è¡¨æ‰«æ)
-- 2. SELECT * FROM orders WHERE phy_id = 'DEV001' ORDER BY created_at DESC LIMIT 100
--    (ç¼ºå°‘å¤åˆç´¢å¼•)
-- 3. SELECT * FROM cmd_logs WHERE device_id = 123 AND created_at BETWEEN ... 
--    (ç´¢å¼•é€‰æ‹©æ€§å·®)
```

---

### 3.2 ä¼˜åŒ–æ–¹æ¡ˆ

#### 3.2.1 æ·»åŠ ç´¢å¼•

```sql
-- db/migrations/0006_query_optimization_up.sql

-- 1. è®¾å¤‡æœ€è¿‘å¿ƒè·³æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_devices_last_seen 
ON devices(last_seen_at DESC) 
WHERE last_seen_at IS NOT NULL;

-- 2. è®¢å•æŸ¥è¯¢ä¼˜åŒ–ï¼ˆå¤åˆç´¢å¼•ï¼‰
CREATE INDEX CONCURRENTLY idx_orders_phy_created 
ON orders(phy_id, created_at DESC);

-- 3. å‘½ä»¤æ—¥å¿—æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_cmd_logs_device_created 
ON cmd_logs(device_id, created_at DESC);

-- 4. ä¸‹è¡Œé˜Ÿåˆ—çŠ¶æ€ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_outbound_status_priority 
ON outbound_queue(status, priority DESC, created_at) 
WHERE status IN (0, 1);

-- 5. ç«¯å£çŠ¶æ€æŸ¥è¯¢ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_ports_device_no 
ON ports(device_id, port_no);
```

#### 3.2.2 æŸ¥è¯¢é‡å†™

```go
// ä¼˜åŒ–å‰ï¼š
const slowQuery = `
    SELECT * FROM devices 
    WHERE last_seen_at > NOW() - INTERVAL '5 minutes'
    ORDER BY last_seen_at DESC
`
// é—®é¢˜ï¼šSELECT *ï¼ŒæŸ¥è¯¢æ‰€æœ‰å­—æ®µ

// ä¼˜åŒ–åï¼š
const fastQuery = `
    SELECT id, phy_id, protocol, last_seen_at 
    FROM devices 
    WHERE last_seen_at > NOW() - INTERVAL '5 minutes'
    ORDER BY last_seen_at DESC
    LIMIT 1000
`
// æ”¹è¿›ï¼šæŒ‡å®šå­—æ®µï¼Œæ·»åŠ LIMIT
```

#### 3.2.3 è¿æ¥æ± ä¼˜åŒ–

```go
// internal/storage/pg/pool.go

func NewPool(cfg *Config) (*pgxpool.Pool, error) {
    config, err := pgxpool.ParseConfig(cfg.DSN)
    if err != nil {
        return nil, err
    }

    // ä¼˜åŒ–è¿æ¥æ± é…ç½®
    config.MaxConns = 20                          // æœ€å¤§è¿æ¥æ•°ï¼ˆæå‡è‡ª10ï¼‰
    config.MinConns = 5                           // æœ€å°è¿æ¥æ•°ï¼ˆé¢„çƒ­ï¼‰
    config.MaxConnLifetime = 1 * time.Hour        // è¿æ¥æœ€å¤§ç”Ÿå‘½å‘¨æœŸ
    config.MaxConnIdleTime = 30 * time.Minute     // ç©ºé—²è¿æ¥è¶…æ—¶
    config.HealthCheckPeriod = 1 * time.Minute    // å¥åº·æ£€æŸ¥å‘¨æœŸ
    
    // è¿æ¥æ± ç»Ÿè®¡
    config.ConnConfig.OnNotice = func(conn *pgx.PgConn, n *pgconn.Notice) {
        log.Info("postgres notice", zap.String("message", n.Message))
    }

    return pgxpool.NewWithConfig(context.Background(), config)
}
```

#### 3.2.4 æŸ¥è¯¢ç»“æœç¼“å­˜

```go
// internal/storage/pg/cached_repo.go

type CachedRepository struct {
    *Repository
    cache *ristretto.Cache
}

func NewCachedRepository(repo *Repository) *CachedRepository {
    cache, _ := ristretto.NewCache(&ristretto.Config{
        NumCounters: 1e7,     // 1000ä¸‡è®¡æ•°å™¨
        MaxCost:     100 << 20, // 100MB
        BufferItems: 64,
    })

    return &CachedRepository{
        Repository: repo,
        cache:      cache,
    }
}

// GetDeviceByPhyID å¸¦ç¼“å­˜çš„æŸ¥è¯¢
func (r *CachedRepository) GetDeviceByPhyID(ctx context.Context, phyID string) (*Device, error) {
    // 1. æŸ¥è¯¢ç¼“å­˜
    cacheKey := fmt.Sprintf("device:%s", phyID)
    if val, found := r.cache.Get(cacheKey); found {
        return val.(*Device), nil
    }

    // 2. æŸ¥è¯¢æ•°æ®åº“
    device, err := r.Repository.GetDeviceByPhyID(ctx, phyID)
    if err != nil {
        return nil, err
    }

    // 3. å†™å…¥ç¼“å­˜ï¼ˆ5åˆ†é’ŸTTLï¼‰
    r.cache.SetWithTTL(cacheKey, device, 1, 5*time.Minute)

    return device, nil
}
```

---

### 3.3 æ€§èƒ½å¯¹æ¯”

| æŸ¥è¯¢ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-----|--------|--------|------|
| åœ¨çº¿è®¾å¤‡åˆ—è¡¨ | 500ms | 50ms | 10x |
| è®¢å•æŸ¥è¯¢ï¼ˆå•è®¾å¤‡ï¼‰ | 200ms | 20ms | 10x |
| å‘½ä»¤æ—¥å¿—æŸ¥è¯¢ | 300ms | 30ms | 10x |
| ç«¯å£çŠ¶æ€æŸ¥è¯¢ | 100ms | 10ms | 10x |

---

## 4ï¸âƒ£ ä»»åŠ¡4: å¥åº·æ£€æŸ¥æ·±åº¦å¢å¼º

### 4.1 å½“å‰é—®é¢˜

```go
// internal/health/ready.go
func (r *Readiness) Ready() bool {
    return r.dbReady.Load() && r.tcpReady.Load()
}

// é—®é¢˜ï¼š
// âŒ åªæ£€æŸ¥å¯åŠ¨çŠ¶æ€ï¼Œä¸æ£€æŸ¥è¿è¡ŒçŠ¶æ€
// âŒ æ•°æ®åº“è¿æ¥æ–­å¼€åä»è¿”å›true
// âŒ Redisæ•…éšœæ—¶æ— æ³•æ„ŸçŸ¥
// âŒ Outboundé˜Ÿåˆ—ç§¯å‹æ— æ³•æ„ŸçŸ¥
```

---

### 4.2 æ·±åº¦å¥åº·æ£€æŸ¥

#### 4.2.1 å¥åº·æ£€æŸ¥æ¥å£

```go
// internal/health/checker.go
package health

import (
    "context"
    "time"
)

// Status å¥åº·çŠ¶æ€
type Status string

const (
    StatusHealthy   Status = "healthy"
    StatusDegraded  Status = "degraded"  // é™çº§
    StatusUnhealthy Status = "unhealthy"
)

// CheckResult æ£€æŸ¥ç»“æœ
type CheckResult struct {
    Status  Status                 `json:"status"`
    Message string                 `json:"message,omitempty"`
    Details map[string]interface{} `json:"details,omitempty"`
    Latency time.Duration          `json:"latency"`
}

// Checker å¥åº·æ£€æŸ¥å™¨æ¥å£
type Checker interface {
    Name() string
    Check(ctx context.Context) CheckResult
}
```

#### 4.2.2 å„ç»„ä»¶å¥åº·æ£€æŸ¥

```go
// internal/health/checkers/database.go

// DatabaseChecker æ•°æ®åº“å¥åº·æ£€æŸ¥
type DatabaseChecker struct {
    pool *pgxpool.Pool
}

func (c *DatabaseChecker) Name() string {
    return "database"
}

func (c *DatabaseChecker) Check(ctx context.Context) CheckResult {
    start := time.Now()
    
    // 1. Pingæµ‹è¯•
    if err := c.pool.Ping(ctx); err != nil {
        return CheckResult{
            Status:  StatusUnhealthy,
            Message: fmt.Sprintf("ping failed: %v", err),
            Latency: time.Since(start),
        }
    }

    // 2. è·å–è¿æ¥æ± çŠ¶æ€
    stats := c.pool.Stat()
    
    // 3. æ£€æŸ¥è¿æ¥æ± å¥åº·åº¦
    utilization := float64(stats.AcquiredConns()) / float64(stats.MaxConns())
    
    status := StatusHealthy
    if utilization > 0.9 {
        status = StatusDegraded
    }

    return CheckResult{
        Status:  status,
        Message: fmt.Sprintf("%.1f%% utilization", utilization*100),
        Details: map[string]interface{}{
            "total_conns":    stats.TotalConns(),
            "idle_conns":     stats.IdleConns(),
            "acquired_conns": stats.AcquiredConns(),
            "max_conns":      stats.MaxConns(),
        },
        Latency: time.Since(start),
    }
}

// internal/health/checkers/redis.go

// RedisChecker Rediså¥åº·æ£€æŸ¥
type RedisChecker struct {
    client *redis.Client
}

func (c *RedisChecker) Check(ctx context.Context) CheckResult {
    start := time.Now()
    
    // Pingæµ‹è¯•
    if err := c.client.Ping(ctx).Err(); err != nil {
        return CheckResult{
            Status:  StatusUnhealthy,
            Message: fmt.Sprintf("ping failed: %v", err),
            Latency: time.Since(start),
        }
    }

    // è·å–Info
    info, err := c.client.Info(ctx, "stats").Result()
    if err != nil {
        return CheckResult{
            Status:  StatusDegraded,
            Message: fmt.Sprintf("info failed: %v", err),
            Latency: time.Since(start),
        }
    }

    // è§£æå†…å­˜ä½¿ç”¨ç‡
    // ... è§£æinfo

    return CheckResult{
        Status:  StatusHealthy,
        Latency: time.Since(start),
    }
}

// internal/health/checkers/outbound.go

// OutboundChecker ä¸‹è¡Œé˜Ÿåˆ—å¥åº·æ£€æŸ¥
type OutboundChecker struct {
    queue *outbound.RedisQueue
}

func (c *OutboundChecker) Check(ctx context.Context) CheckResult {
    start := time.Now()
    
    // 1. è·å–é˜Ÿåˆ—é•¿åº¦
    queueSize, _ := c.queue.GetQueueSize(ctx)
    pendingSize, _ := c.queue.GetPendingSize(ctx)
    deadSize, _ := c.queue.GetDeadSize(ctx)

    // 2. åˆ¤æ–­å¥åº·çŠ¶æ€
    status := StatusHealthy
    message := "ok"

    if queueSize > 10000 {
        status = StatusDegraded
        message = "queue backlog"
    }

    if deadSize > 1000 {
        status = StatusUnhealthy
        message = "too many dead messages"
    }

    return CheckResult{
        Status:  status,
        Message: message,
        Details: map[string]interface{}{
            "queue_size":   queueSize,
            "pending_size": pendingSize,
            "dead_size":    deadSize,
        },
        Latency: time.Since(start),
    }
}

// internal/health/checkers/tcp.go

// TCPChecker TCPæœåŠ¡å™¨å¥åº·æ£€æŸ¥
type TCPChecker struct {
    server *tcpserver.Server
}

func (c *TCPChecker) Check(ctx context.Context) CheckResult {
    start := time.Now()
    
    // è·å–è¿æ¥æ•°
    activeConns := c.server.ActiveConnections()
    maxConns := c.server.MaxConnections()

    // è®¡ç®—åˆ©ç”¨ç‡
    utilization := float64(activeConns) / float64(maxConns)

    status := StatusHealthy
    if utilization > 0.9 {
        status = StatusDegraded
    }

    return CheckResult{
        Status:  status,
        Message: fmt.Sprintf("%.1f%% connections", utilization*100),
        Details: map[string]interface{}{
            "active_connections": activeConns,
            "max_connections":    maxConns,
        },
        Latency: time.Since(start),
    }
}
```

#### 4.2.3 å¥åº·æ£€æŸ¥èšåˆå™¨

```go
// internal/health/aggregator.go

type Aggregator struct {
    checkers []Checker
}

func NewAggregator(checkers ...Checker) *Aggregator {
    return &Aggregator{checkers: checkers}
}

// CheckAll æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥
func (a *Aggregator) CheckAll(ctx context.Context) map[string]CheckResult {
    results := make(map[string]CheckResult)
    
    for _, checker := range a.checkers {
        results[checker.Name()] = checker.Check(ctx)
    }
    
    return results
}

// OverallStatus æ€»ä½“å¥åº·çŠ¶æ€
func (a *Aggregator) OverallStatus(ctx context.Context) Status {
    results := a.CheckAll(ctx)
    
    unhealthyCount := 0
    degradedCount := 0
    
    for _, result := range results {
        switch result.Status {
        case StatusUnhealthy:
            unhealthyCount++
        case StatusDegraded:
            degradedCount++
        }
    }
    
    // ä»»ä½•ç»„ä»¶Unhealthyï¼Œæ•´ä½“Unhealthy
    if unhealthyCount > 0 {
        return StatusUnhealthy
    }
    
    // ä»»ä½•ç»„ä»¶Degradedï¼Œæ•´ä½“Degraded
    if degradedCount > 0 {
        return StatusDegraded
    }
    
    return StatusHealthy
}
```

#### 4.2.4 å¥åº·æ£€æŸ¥HTTPæ¥å£

```go
// internal/httpserver/health.go

func RegisterHealthRoutes(r *gin.Engine, aggregator *health.Aggregator) {
    // 1. Readinessæ¢é’ˆï¼ˆK8sä½¿ç”¨ï¼‰
    r.GET("/health/ready", func(c *gin.Context) {
        ctx := c.Request.Context()
        status := aggregator.OverallStatus(ctx)
        
        if status == health.StatusUnhealthy {
            c.JSON(503, gin.H{
                "status": "unhealthy",
                "ready":  false,
            })
            return
        }
        
        c.JSON(200, gin.H{
            "status": status,
            "ready":  true,
        })
    })
    
    // 2. Livenessæ¢é’ˆï¼ˆK8sä½¿ç”¨ï¼‰
    r.GET("/health/live", func(c *gin.Context) {
        // ç®€å•æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æ´»ç€
        c.JSON(200, gin.H{"alive": true})
    })
    
    // 3. è¯¦ç»†å¥åº·æ£€æŸ¥
    r.GET("/health", func(c *gin.Context) {
        ctx := c.Request.Context()
        results := aggregator.CheckAll(ctx)
        overall := aggregator.OverallStatus(ctx)
        
        code := 200
        if overall == health.StatusUnhealthy {
            code = 503
        } else if overall == health.StatusDegraded {
            code = 200  // Degradedä»è¿”å›200
        }
        
        c.JSON(code, gin.H{
            "status":  overall,
            "checks":  results,
            "timestamp": time.Now(),
        })
    })
}
```

---

### 4.3 å¥åº·æ£€æŸ¥å“åº”ç¤ºä¾‹

```json
// GET /health

{
  "status": "healthy",
  "timestamp": "2025-10-05T12:00:00Z",
  "checks": {
    "database": {
      "status": "healthy",
      "message": "45.2% utilization",
      "details": {
        "total_conns": 10,
        "idle_conns": 5,
        "acquired_conns": 5,
        "max_conns": 20
      },
      "latency": "5ms"
    },
    "redis": {
      "status": "healthy",
      "latency": "2ms"
    },
    "outbound": {
      "status": "degraded",
      "message": "queue backlog",
      "details": {
        "queue_size": 15000,
        "pending_size": 200,
        "dead_size": 50
      },
      "latency": "3ms"
    },
    "tcp": {
      "status": "healthy",
      "message": "65.5% connections",
      "details": {
        "active_connections": 6550,
        "max_connections": 10000
      },
      "latency": "1ms"
    }
  }
}
```

---

## ğŸ“Š Week 2 æ€»ä½“è§„åˆ’

### æ—¶é—´è¡¨

```
Week 2.1 (Day 1-5):
â”œâ”€ Day 1: Redis Outboundè®¾è®¡å’Œå¼€å‘ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
â”œâ”€ Day 2: Redis Outboundæµ‹è¯•å’Œä¼˜åŒ–
â”œâ”€ Day 3: æ•°æ®è¿ç§»å’ŒåŒå†™å®ç°
â”œâ”€ Day 4: é™æµå™¨å’Œç†”æ–­å™¨å¼€å‘
â””â”€ Day 5: æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–

Week 2.2 (Day 6-10):
â”œâ”€ Day 6: å¥åº·æ£€æŸ¥å¢å¼º
â”œâ”€ Day 7: é›†æˆæµ‹è¯•
â”œâ”€ Day 8: æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜
â”œâ”€ Day 9: ç°åº¦å‘å¸ƒåˆ°æµ‹è¯•ç¯å¢ƒ
â””â”€ Day 10: ç›‘æ§å’Œæ–‡æ¡£å®Œå–„

Week 2.3 (Day 11-14):
â”œâ”€ Day 11-12: ç”Ÿäº§ç¯å¢ƒç°åº¦ï¼ˆ10% â†’ 50% â†’ 100%ï¼‰
â”œâ”€ Day 13: ç›‘æ§å’Œé—®é¢˜ä¿®å¤
â””â”€ Day 14: æ€»ç»“å’Œæ¸…ç†
```

---

### éªŒæ”¶æ ‡å‡†

#### åŠŸèƒ½éªŒæ”¶

- [ ] Redis Outbound TPS â‰¥ 1000/s
- [ ] è¿æ¥æ•°é™æµç”Ÿæ•ˆï¼ˆè¾¾åˆ°ä¸Šé™æ—¶æ‹’ç»ï¼‰
- [ ] ç†”æ–­å™¨çŠ¶æ€æ­£ç¡®è½¬æ¢
- [ ] æ•°æ®åº“æŸ¥è¯¢P99 < 100ms
- [ ] å¥åº·æ£€æŸ¥è¦†ç›–æ‰€æœ‰ç»„ä»¶

#### æ€§èƒ½éªŒæ”¶

- [ ] Outboundå»¶è¿Ÿé™ä½è‡³ < 10ms
- [ ] APIå“åº”æ—¶é—´é™ä½50%
- [ ] æ”¯æŒ10000+å¹¶å‘è¿æ¥
- [ ] Rediså†…å­˜ä½¿ç”¨ < 1GB

#### ç¨³å®šæ€§éªŒæ”¶

- [ ] å‹æµ‹24å°æ—¶æ— å´©æºƒ
- [ ] é”™è¯¯ç‡ < 0.1%
- [ ] å†…å­˜æ— æ³„éœ²
- [ ] ç†”æ–­å™¨å‘Šè­¦æ­£å¸¸

---

### é£é™©è¯„ä¼°

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|-----|------|------|---------|
| Redisæ€§èƒ½ä¸è¾¾é¢„æœŸ | ä½ | é«˜ | åŒå†™æ–¹æ¡ˆï¼Œå¯å›æ»šPG |
| é™æµå¯¼è‡´ä¸šåŠ¡é˜»å¡ | ä¸­ | ä¸­ | é…ç½®åˆç†é˜ˆå€¼ï¼Œç›‘æ§å‘Šè­¦ |
| æ•°æ®åº“è¿ç§»å¤±è´¥ | ä½ | é«˜ | åœ¨çº¿DDLï¼Œåˆ†æ­¥æ‰§è¡Œ |
| ç†”æ–­å™¨è¯¯è§¦å‘ | ä¸­ | ä¸­ | è°ƒæ•´é˜ˆå€¼ï¼Œå¢åŠ æ—¥å¿— |

---

### ç›‘æ§æŒ‡æ ‡

#### æ ¸å¿ƒæŒ‡æ ‡

```
# Outbound
outbound_queue_size          # é˜Ÿåˆ—é•¿åº¦
outbound_throughput          # ååé‡ï¼ˆmsg/sï¼‰
outbound_latency_seconds     # å»¶è¿Ÿåˆ†å¸ƒ

# é™æµ
tcp_connections_rejected     # æ‹’ç»è¿æ¥æ•°
tcp_connections_active       # æ´»è·ƒè¿æ¥æ•°
rate_limiter_allowed         # é€Ÿç‡é™æµé€šè¿‡æ•°

# ç†”æ–­å™¨
circuit_breaker_state        # ç†”æ–­å™¨çŠ¶æ€ï¼ˆ0=closed,1=open,2=half-openï¼‰
circuit_breaker_failures     # å¤±è´¥æ¬¡æ•°
circuit_breaker_trips        # ç†”æ–­æ¬¡æ•°

# æ•°æ®åº“
db_query_latency_seconds     # æŸ¥è¯¢å»¶è¿Ÿ
db_pool_connections          # è¿æ¥æ± çŠ¶æ€
```

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | Week 2ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|-----|------|-----------|---------|
| Outbound TPS | 100 | 1000 | å‹æµ‹ |
| API P99å»¶è¿Ÿ | 200ms | 100ms | APM |
| å¹¶å‘è¿æ¥ | 1000 | 10000 | è´Ÿè½½æµ‹è¯• |
| ç³»ç»Ÿå¯ç”¨æ€§ | 99% | 99.5% | ç›‘æ§ç»Ÿè®¡ |

### ä¸šåŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | æ”¹è¿›å‰ | ç›®æ ‡ |
|-----|--------|------|
| ä¸‹è¡Œæ¨é€å»¶è¿Ÿ | 1-5ç§’ | <1ç§’ |
| å‘½ä»¤å¤±è´¥ç‡ | 1% | 0.5% |
| èµ„æºä½¿ç”¨ç‡ | é«˜ | ä¸­ |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [Rate Limiting Strategies](https://konghq.com/blog/how-to-design-a-scalable-rate-limiting-algorithm)
- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)
- [PostgreSQL Performance Tuning](https://www.postgresql.org/docs/current/performance-tips.html)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ¶å®šæ—¥æœŸ**: 2025-10-05  
**å®¡æ ¸çŠ¶æ€**: â³ å¾…å®¡æ ¸  
**ä¸‹ä¸€æ­¥**: ç­‰å¾…æŠ€æœ¯è¯„å®¡ä¼šè®®
